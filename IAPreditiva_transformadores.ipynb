{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "3n_ezdDr_kUP",
        "outputId": "9b2ab73a-ab64-4548-aa66-5441b1b27afa"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'sample_data/dataset_transformadores_mes_expandido.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-97150956>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# 1. Carregar e preparar dados\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sample_data/dataset_transformadores_mes_expandido.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Data_Hora'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Data_Hora'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sample_data/dataset_transformadores_mes_expandido.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import classification_report, precision_recall_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.inspection import permutation_importance\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Carregar e preparar dados\n",
        "df = pd.read_csv(\"sample_data/dataset_transformadores_mes_expandido.csv\")\n",
        "df['Data_Hora'] = pd.to_datetime(df['Data_Hora'])\n",
        "\n",
        "# Engenharia de features\n",
        "df['Hora'] = df['Data_Hora'].dt.hour\n",
        "df['Dia_Semana'] = df['Data_Hora'].dt.dayofweek\n",
        "df['Variacao_Tensao_1h'] = df.groupby('Transformador')['Tensao_RMS'].diff(1)\n",
        "df['Razao_Tensao_Consumo'] = df['Tensao_RMS'] / (df['Consumo_kWh'] + 1e-10)\n",
        "\n",
        "# Criar labels\n",
        "df['Anomalia_Real'] = np.where(\n",
        "    (df['Tensao_RMS'] < 210) | (df['Tensao_RMS'] > 240) |\n",
        "    (df['Temp_Transformador_C'] > 90),\n",
        "    1, 0\n",
        ")\n",
        "\n",
        "# Remover possíveis NaNs\n",
        "df.dropna(subset=['Variacao_Tensao_1h'], inplace=True)\n",
        "\n",
        "# Seleção de features\n",
        "features = ['Tensao_RMS', 'Temp_Transformador_C', 'Consumo_kWh', 'Variacao_Tensao_1h', 'Razao_Tensao_Consumo']\n",
        "\n",
        "# 2. Pré-processamento\n",
        "scaler = RobustScaler()\n",
        "X = scaler.fit_transform(df[features])\n",
        "y = df['Anomalia_Real']\n",
        "\n",
        "# 3. Modelo Final\n",
        "final_model = IsolationForest(\n",
        "    n_estimators=200,\n",
        "    max_samples=512,\n",
        "    contamination=0.05,\n",
        "    max_features=0.8,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "final_model.fit(X)\n",
        "\n",
        "# 4. Análise de Threshold Ótimo\n",
        "y_scores = final_model.decision_function(X)\n",
        "precision, recall, thresholds = precision_recall_curve(y, -y_scores)\n",
        "\n",
        "# Encontrar threshold para recall ~0.7\n",
        "target_recall = 0.7\n",
        "optimal_idx = np.argmin(np.abs(recall - target_recall))\n",
        "optimal_threshold = -thresholds[optimal_idx]\n",
        "\n",
        "# 5. Métricas Finais\n",
        "y_pred_final = np.where(y_scores < optimal_threshold, 1, 0)\n",
        "print(\"\\n🔍 Métricas Finais:\")\n",
        "print(classification_report(y, y_pred_final))\n",
        "\n",
        "# 6. Feature Importance via Permutation Importance com anomaly scores\n",
        "result = permutation_importance(\n",
        "    final_model, X, y_scores,\n",
        "    n_repeats=10, random_state=42, n_jobs=-1,\n",
        "    scoring='neg_mean_absolute_error'\n",
        ")\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Importance': result.importances_mean\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\n📌 Importância das Features:\")\n",
        "print(feature_importance)\n",
        "\n",
        "# 7. Visualizações\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(-y_scores, bins=50, kde=True)\n",
        "plt.axvline(x=optimal_threshold, color='r', linestyle='--')\n",
        "plt.title(\"Distribuição de Scores de Anomalia\")\n",
        "plt.xlabel('Anomaly Score')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(recall, precision)\n",
        "plt.axvline(x=target_recall, color='r', linestyle='--')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Curva Precision-Recall')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Adiciona as previsões e scores ao DataFrame original\n",
        "df['Anomaly_Score'] = -y_scores  # negativo porque maiores valores = mais anômalos no Isolation Forest\n",
        "df['Anomalia_Prevista'] = y_pred_final\n",
        "\n",
        "# Ranqueia transformadores pelos mais críticos (mais anomalias detectadas)\n",
        "ranking_transformadores = df.groupby('Transformador').agg({\n",
        "    'Anomalia_Prevista': 'sum',\n",
        "    'Anomaly_Score': 'mean'\n",
        "}).rename(columns={\n",
        "    'Anomalia_Prevista': 'Qtd_Anomalias',\n",
        "    'Anomaly_Score': 'Score_Medio'\n",
        "}).sort_values(['Qtd_Anomalias', 'Score_Medio'], ascending=[False, False])\n",
        "\n",
        "# Exibe ranking\n",
        "print(\"\\n📊 Ranking de Transformadores mais Críticos para Preventiva:\")\n",
        "print(ranking_transformadores.head(10))  # top 10\n",
        "\n",
        "# Opcional: visualizar ranking\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(\n",
        "    data=ranking_transformadores.reset_index().head(10),\n",
        "    x='Transformador', y='Qtd_Anomalias', palette='Reds_r'\n",
        ")\n",
        "plt.title('Top 10 Transformadores mais Críticos (Qtd de Anomalias)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Qtd de Anomalias Detectadas')\n",
        "plt.xlabel('Transformador')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ]
}